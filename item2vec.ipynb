{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Fashion Recommender System from Learned Embeddings\n",
    "```\n",
    "Seyed Saeid Masoumzadeh \n",
    "Senior Data Scientist @ Lyst\n",
    "Open Data Science Conference (ODSC) London - 16th June 2022\n",
    "```\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "**ADD Images to the folder**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec - SkipGram architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The Skip-gram model architecture usually tries to predict the probability of the context words (surrounding words) given a target word.\n",
    "\n",
    "```\n",
    "- A shallow network including just one hidden layer\n",
    "- Input size is equal to the number of unique words/phrases we have in our text/corpus\n",
    "- Output size is also equal to the number of unique words/phrases we have in our text/corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec - Data Sampling \n",
    "```\n",
    "the word2vec data sampling using a sliding window strategy whereby the window size specifies how many next or previous token must be considered to be paired with a given token in the window.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a method to do sampling using sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(sequence, window_size):\n",
    "    \"\"\"\n",
    "    This function provides a sampling using a window strategy, the window moves on the sequence\n",
    "    of link_ids and the positives are selected in the scope of the window. e.g, if a list of sequence is\n",
    "    [1,2,3,4] and the window is 1, the samples are [(1,2), (2,1), (2,3), (3,2), (3,4), (4,3)].\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_tokens = len(sequence)\n",
    "    samples = []\n",
    "    for i in range(number_of_tokens):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + list(\n",
    "            range(i + 1, min(number_of_tokens, i + window_size + 1))\n",
    "        )\n",
    "        for j in nbr_inds:\n",
    "            samples.append((sequence[i], sequence[j]))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sequence = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "sample_data(sequence, window_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "for the sample (quick, brown), the input to word2vec model is a one-hot vector where all the cells are zeros except the cell pointing to the word quick,  which is initialized with 1. The output is again a one-hot vector where all the cells are zeros except the cell pointing to the word brown. Briefly speaking word2vec can be considered as a multi class classifier and can be solved using a sampled softmax loss.  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import glob\n",
    "import re\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from rpforest import RPForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "A sentence is a sequence of words, and Word2vec using skip-gram model tries to find the probability of the surrounding words given a word. Is it a concept that we can apply on the other sequences?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading session data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an anonymized data, showing the users' interactions in terms of clicking on the items, for example in a fashion platform like Lyst. \n",
    "- the session_id represnts a user\n",
    "- the product_id represents a fashion product/clothing item has been clicked by the user\n",
    "- the event_time_stamp is the time the click event occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"data/data.parquet\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting by event time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values('event_time_stamp')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the sequence of clicks\n",
    "```\n",
    "grouping the data by session_id allows us to build the product_id sequences which have been clicked by the users. Each sequence has been sorted by the time the click occurred as a result of the previous sorting logic.\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['product_id'] = data['product_id'].astype(str)\n",
    "session_seq = data.groupby('session_id')['product_id'].apply(list).reset_index(\n",
    ").rename(columns={'product_id':\"sequence_of_clicks\"})\n",
    "session_seq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data exploration on sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_seq['sequence_length'] = session_seq['sequence_of_clicks'].apply(lambda x: len(x))\n",
    "session_seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_seq['sequence_length'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_seq['sequence_length'].quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_seq = session_seq[session_seq['sequence_length'] <= session_seq['sequence_length'].quantile(0.95)]\n",
    "session_seq = session_seq[session_seq['sequence_length'] >= 2]\n",
    "session_seq['sequence_length'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_seq['sequence_length'].value_counts().to_frame().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data(['1463503', '1418365', '1531480'],  2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SkipGram (using fasttext) on the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_params = {\n",
    "            \"model\": \"skipgram\",\n",
    "            \"lr\": 0.05,\n",
    "            \"dim\": 100,\n",
    "            \"ws\": 3,\n",
    "            \"epoch\": 100,\n",
    "            \"minCount\": 1,\n",
    "            \"minn\": 3,\n",
    "            \"maxn\": 0,\n",
    "            \"neg\": 5,\n",
    "            \"wordNgrams\": 1,\n",
    "            \"loss\": \"ns\",\n",
    "            \"bucket\": 2000000,\n",
    "            \"thread\": 24,\n",
    "            \"lrUpdateRate\": 100,\n",
    "            \"t\": 0.0001,\n",
    "            \"verbose\": 2,\n",
    "        }\n",
    "sequence_txt_file = 'data/seq.txt'\n",
    "sequence = [' '.join(x) for x in session_seq['sequence_of_clicks'].values]\n",
    "np.savetxt(sequence_txt_file, sequence, fmt=\"%s\", encoding=\"utf-8\")\n",
    "model = fasttext.train_unsupervised(sequence_txt_file, **fasttext_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.vstack([model[x] for x in model.words]).astype(\"double\")\n",
    "vocabs = model.words\n",
    "\n",
    "vectors_dict = dict(zip(vocabs, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dict['1531480']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"\n",
    "    Takes 2 ndarray and  a, b and returns the cosine similarity according\n",
    "    to the definition of the dot product.\n",
    "        a should be a single 1-d array\n",
    "        b should be a 2-d array\n",
    "    \"\"\"\n",
    "\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b, axis=1)\n",
    "    return np.dot(a, b.T) / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a hash table for (product_id, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('product_images/*.jpeg')\n",
    "file_dict = {}\n",
    "for file in files:\n",
    "    result = re.search('images/(.*).jpeg', file)\n",
    "    file_dict[result.group(1)] = file   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similar items to a given item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = cos_sim(vectors_dict['1556752'], vectors)\n",
    "sims = sorted(zip(vocabs, sims), key=lambda x: x[1], reverse=True)[:9]\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for product_id, sim in sims: \n",
    "    images.append(file_dict[product_id]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(images[0], cv2.IMREAD_COLOR)\n",
    "plt.imshow(img[:,:,::-1])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "i = 1\n",
    "for image in images[1:]:\n",
    "    img =  cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "    ax = fig.add_subplot(3, 3, i)\n",
    "    plt.imshow(img[:,:,::-1])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Nearest Neighbor (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Finding points in a high-dimensional space that are close to a given query point in a fast but approximate manner.\n",
    "\n",
    "In each tree, the set of training points is recursively partitioned into smaller and smaller subsets until a leaf node of at most M points is reached. Each partition is based on the cosine of the angle the points make with a randomly drawn hyperplane: points whose angle is smaller than the median angle fall in the left partition, and the remaining points fall in the right partition.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/rpforest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train rpforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpf_model = RPForest(leaf_size=50, no_trees=10)\n",
    "rpf_model.fit(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similar items by making query to ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_index = rpf_model.query(vectors_dict['1556752'], 9)\n",
    "sims = [vocabs[i] for i in sims_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for product_id in sims:\n",
    "    try:\n",
    "        images.append(file_dict[product_id])\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(images[0], cv2.IMREAD_COLOR)\n",
    "plt.imshow(img[:,:,::-1])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "i = 1\n",
    "for image in images[1:]:\n",
    "    img =  cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "    ax = fig.add_subplot(3, 3, i)\n",
    "    plt.imshow(img[:,:,::-1])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold start problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cold start heppens in this steup when an item have some content information but no interactions are present\n",
    " - **An efficnet solution would be using a triplet neural network**\n",
    " \n",
    "Triplet NN helps us to learn distributed embedding by the notion of similarity and dissimilarity. It's a kind of neural network architecture where multiple parallel networks are trained that share weights among each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/triplet_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/triplet_rec_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Anchor, Positive and Negative\n",
    "sampling anchor and positive is the same as what has been explained in Item2vec approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data(['1463503', '1418365', '1531480'],  2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about negatives???\n",
    "the negatives are sampled randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L(a, p, n) = max(0, D(a, p) — D(a, n) + margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(a, p, n, margin):\n",
    "    dist_a_p = 1 - cos_sim(a, p.reshape(1, -1))[0]\n",
    "    dist_a_n = 1 - cos_sim(a, n.reshape(1, -1))[0]\n",
    "    return max(dist_a_p - dist_a_n + margin, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher margin, the softer negatives contributes into the cost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = vectors_dict['1556752']\n",
    "p = vectors_dict['1387755']\n",
    "s_n = vectors_dict['1418365'] #soft_negative\n",
    "s_h = vectors_dict['1451117'] #hard_negative\n",
    "\n",
    "triplet_loss(a, p, s_h, margin= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
